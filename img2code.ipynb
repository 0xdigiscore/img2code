{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T04:41:21.775119Z",
     "start_time": "2018-05-26T04:41:21.765613Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM, concatenate , Input, Reshape, Dense, Flatten\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "## 神经网洛生成代码\n",
    "\n",
    "## 目标\n",
    "\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/b03c0f781f2807f0c025aaf2678d852d.png)\n",
    "\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/83b33808047ae3fec5ed282e5638800d.png)\n",
    "\n",
    "## 框架\n",
    "\n",
    "结合CNN与LSTM的方法，使用cnn抽取图像特征，使用RNN学习文本和序列规律，把两组上下文集成起来，我们就有信息知道一张设计图原型的语义，每个语义对应DSL,最后根据DSL生成源代码。\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/c60bc19ada92800c742b498395101688.png)\n",
    "\n",
    "### 训练数据\n",
    "\n",
    "训练数据有两部分：\n",
    "\n",
    "* GUI原型图\n",
    "\n",
    "* DSL上下文\n",
    "\n",
    "### pixcode内部网络\n",
    "\n",
    "* 一个CNN网络来理解GUI内容，获得GUI图像特征。\n",
    "* 一个LSTM网络来理解DSL上下文的基本规律，a单词token产生下一个b单词token的规律（不包含与原型图的关系）\n",
    "两层各128个单元的LSTM模块\n",
    "* 另一个LSTM‘ 用来理解DSL与对应原型图的关系，x原型图应该生成怎样的上下文token c?\n",
    "两层各512个单元的LSTM模块\n",
    "\n",
    "### 训练公式\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/1e03bce426eb5be242527488282638b8.png)\n",
    "\n",
    "通过拼接cnn输出p和LSTM的隐层输出q，合成为r作为原型图和DSK相关性的依据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最终目标\n",
    "\n",
    "#### 使用生成对抗网路来生成原型稿\n",
    "\n",
    "最终的目标我们将通过手绘简笔画，生成视觉原型稿，再由对视觉原型稿生成dsl，通过对dsl生成各个平台android，ios，html代码。\n",
    "\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/0dabad9cd2a113997406f8cc9e10f952.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T04:43:25.069872Z",
     "start_time": "2018-05-26T04:41:21.776691Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  准备图像特征数据\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load the images and preprocess them for inception-resnet\n",
    "images = []\n",
    "all_filenames = listdir('images/')\n",
    "all_filenames.sort()\n",
    "for filename in all_filenames:\n",
    "    images.append(img_to_array(load_img('images/'+filename, target_size=(299, 299))))\n",
    "images = np.array(images, dtype=float)\n",
    "images = preprocess_input(images)\n",
    "\n",
    "# Run the images through inception-resnet and extract the features without the classification layer\n",
    "IR2 = InceptionResNetV2(weights='imagenet', include_top=False)\n",
    "features = IR2.predict(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 文本数据预处理\n",
    "#### 1. 简介\n",
    "在进行自然语言处理之前，需要对文本进行处理。 下面将介绍keras提供的预处理包keras.preproceing下的text与序列处理模块sequence模块。\n",
    "单词预处理的整体步骤如下：\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/cf46fe231d7b6133a0fe2b6a35961e5c.png)\n",
    "\n",
    "\n",
    "#### 2.text模块提供的方法\n",
    "* text_to_word_sequence(text,fileter) 可以简单理解此函数功能类str.split\n",
    "* one_hot(text,vocab_size) 基于hash函数(桶大小为vocab_size)，将一行文本转换向量表示（把单词数字化，vocab_size=5表示所有单词全都数字化在5以内）\n",
    "\n",
    "#### 3. text.Tokenizer类\n",
    "这个类用来对文本中的词进行统计计数，生成文档词典，以支持基于词典位序生成文本的向量表示。 \n",
    "init(num_words) 构造函数，传入词典的最大值\n",
    "\n",
    "##### 成员函数\n",
    "* fit_on_text(texts) 使用一系列文档来生成token词典，texts为list类，每个元素为一个文档。\n",
    "* texts_to_sequences(texts) 将多个文档转换为word下标的向量形式,shape为[len(texts)，len(text)] -- (文档数，每条文档的长度)\n",
    "* texts_to_matrix(texts) 将多个文档转换为矩阵表示,shape为[len(texts),num_words]\n",
    "\n",
    "##### 成员变量\n",
    "* document_count 处理的文档数量\n",
    "* word_index 一个dict，保存所有word对应的编号id，从1开始\n",
    "* word_counts 一个dict，保存每个word在所有文档中出现的次数\n",
    "* word_docs 一个dict，保存每个word出现的文档的数量\n",
    "* index_docs 一个dict，保存word的id出现的文档的数量\n",
    "\n",
    "#### 示例\n",
    "```\n",
    "import keras.preprocessing.text as T\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "text1='some thing to eat'\n",
    "text2='some thing to drink'\n",
    "texts=[text1,text2]\n",
    "\n",
    "print T.text_to_word_sequence(text1)  #以空格区分，中文也不例外 ['some', 'thing', 'to', 'eat']\n",
    "print T.one_hot(text1,10)  #[7, 9, 3, 4] -- （10表示数字化向量为10以内的数字）\n",
    "print T.one_hot(text2,10)  #[7, 9, 3, 1]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None) #num_words:None或整数,处理的最大单词数量。少于此数的单词丢掉\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print( tokenizer.word_counts) #[('some', 2), ('thing', 2), ('to', 2), ('eat', 1), ('drink', 1)]\n",
    "print( tokenizer.word_index) #{'some': 1, 'thing': 2,'to': 3 ','eat': 4, drink': 5}\n",
    "print( tokenizer.word_docs) #{'some': 2, 'thing': 2, 'to': 2, 'drink': 1,  'eat': 1}\n",
    "print( tokenizer.index_docs) #{1: 2, 2: 2, 3: 2, 4: 1, 5: 1}\n",
    "\n",
    "# num_words=多少会影响下面的结果，行数=num_words\n",
    "print( tokenizer.texts_to_sequences(texts)) #得到词索引[[1, 2, 3, 4], [1, 2, 3, 5]]\n",
    "print( tokenizer.texts_to_matrix(texts))  # 矩阵化=one_hot\n",
    "[[ 0.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Embending Layer\n",
    "\n",
    "\n",
    "embending层看这篇文档\n",
    "<http://frankchen.xyz/2017/12/18/How-to-Use-Word-Embedding-Layers-for-Deep-Learning-with-Keras/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T04:43:26.056976Z",
     "start_time": "2018-05-26T04:43:25.074190Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 对训练的code数据进行预处理\n",
    "\"\"\"\n",
    "# We will cap each input sequence to 100 tokens\n",
    "max_caption_len = 100\n",
    "# Initialize the function that will create our vocabulary \n",
    "tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "\n",
    "# Read a document and return a string\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# Load all the HTML files\n",
    "X = []\n",
    "all_filenames = listdir('html/')\n",
    "all_filenames.sort()\n",
    "for filename in all_filenames:\n",
    "    X.append(load_doc('html/'+filename))\n",
    "\n",
    "# Create the vocabulary from the html files\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Add +1 to leave space for empty words\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# Translate each word in text file to the matching vocabulary index\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "# The longest HTML file\n",
    "max_length = max(len(s) for s in sequences)\n",
    "\n",
    "# Intialize our final input to the model\n",
    "X, y, image_data = list(), list(), list()\n",
    "for img_no, seq in enumerate(sequences):\n",
    "    for i in range(1, len(seq)):\n",
    "        # Add the entire sequence to the input and only keep the next word for the output\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        # If the sentence is shorter than max_length, fill it up with empty words\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "        # Map the output to one-hot encoding\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        # Add and image corresponding to the HTML file\n",
    "        image_data.append(features[img_no])\n",
    "        # Cut the input sentence to 100 tokens, and add it to the input data\n",
    "        X.append(in_seq[-100:])\n",
    "        y.append(out_seq)\n",
    "\n",
    "X, y, image_data = np.array(X), np.array(y), np.array(image_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo神经网络的架构\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/98a83fb2328b6d98169f2156c149df07.png)\n",
    "\n",
    "### keras 函数式模型来连接网络\n",
    "\n",
    "#### 序贯模型：全连接网络\n",
    "```\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(784,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels)  # starts training\n",
    "```\n",
    "#### 多输入和多输出模型\n",
    "使用函数式模型的一个典型场景是搭建多输入、多输出的模型。\n",
    "\n",
    "考虑这样一个模型。我们希望预测Twitter上一条新闻会被转发和点赞多少次。模型的主要输入是新闻本身，也就是一个词语的序列。但我们还可以拥有额外的输入，如新闻发布的日期等。这个模型的损失函数将由两部分组成，辅助的损失函数评估仅仅基于新闻本身做出预测的情况，主损失函数评估基于新闻和额外信息的预测的情况，即使来自主损失函数的梯度发生弥散，来自辅助损失函数的信息也能够训练Embeddding和LSTM层。在模型中早点使用主要的损失函数是对于深度网络的一个良好的正则方法。总而言之，该模型框图如下：\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/14fd9f9b3e088900016fb901e535dd96.png)\n",
    "让我们用函数式模型来实现这个框图\n",
    "\n",
    "主要的输入接收新闻本身，即一个整数的序列（每个整数编码了一个词）。这些整数位于1到10，000之间（即我们的字典有10，000个词）。这个序列有100个单词。\n",
    "```\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n",
    "# Note that we can name any layer by passing it a \"name\" argument.\n",
    "main_input = Input(shape=(100,), dtype='int32', name='main_input')\n",
    "\n",
    "# This embedding layer will encode the input sequence\n",
    "# into a sequence of dense 512-dimensional vectors.\n",
    "x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n",
    "\n",
    "# A LSTM will transform the vector sequence into a single vector,\n",
    "# containing information about the entire sequence\n",
    "lstm_out = LSTM(32)(x)\n",
    "```\n",
    "然后，我们插入一个额外的损失，使得即使在主损失很高的情况下，LSTM和Embedding层也可以平滑的训练。\n",
    "\n",
    "```\n",
    "auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)\n",
    "```\n",
    "再然后，我们将LSTM与额外的输入数据串联起来组成输入，送入模型中：\n",
    "```\n",
    "auxiliary_input = Input(shape=(5,), name='aux_input')\n",
    "x = keras.layers.concatenate([lstm_out, auxiliary_input])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n",
    "```\n",
    "最后，我们定义整个2输入，2输出的模型：\n",
    "```\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])\n",
    "```\n",
    "模型定义完毕，下一步编译模型。我们给额外的损失赋0.2的权重。我们可以通过关键字参数loss_weights或loss来为不同的输出设置不同的损失函数或权值。这两个参数均可为Python的列表或字典。这里我们给loss传递单个损失函数，这个损失函数会被应用于所有输出上。\n",
    "```\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "              loss_weights=[1., 0.2])\n",
    "```\n",
    "编译完成后```，我们通过传递训练数据和目标值训练该模型：\n",
    "```model.fit([headline_data, additional_data], [labels, labels],\n",
    "          epochs=50, batch_size=32)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T04:43:26.064782Z",
     "start_time": "2018-05-26T04:43:26.059401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n搭建代码生成网络\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "搭建代码生成网络\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-26T04:41:21.771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the encoder\n",
    "image_features = Input(shape=(8, 8, 1536,))\n",
    "image_flat = Flatten()(image_features)\n",
    "image_flat = Dense(128, activation='relu')(image_flat)\n",
    "ir2_out = RepeatVector(max_caption_len)(image_flat)\n",
    "\n",
    "language_input = Input(shape=(max_caption_len,))\n",
    "language_model = Embedding(vocab_size, 200, input_length=max_caption_len)(language_input)\n",
    "language_model = LSTM(256, return_sequences=True)(language_model)\n",
    "language_model = LSTM(256, return_sequences=True)(language_model)\n",
    "language_model = TimeDistributed(Dense(128, activation='relu'))(language_model)\n",
    "\n",
    "# Create the decoder\n",
    "decoder = concatenate([ir2_out, language_model])\n",
    "decoder = LSTM(512, return_sequences=False)(decoder)\n",
    "decoder_output = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[image_features, language_input], outputs=decoder_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-26T04:41:21.776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2306/2306 [==============================] - 74s 32ms/step - loss: 5.9631\n",
      "Epoch 2/500\n",
      "2306/2306 [==============================] - 56s 24ms/step - loss: 5.7329\n",
      "Epoch 3/500\n",
      "2306/2306 [==============================] - 53s 23ms/step - loss: 5.7348\n",
      "Epoch 4/500\n",
      "2306/2306 [==============================] - 53s 23ms/step - loss: 5.7193\n",
      "Epoch 5/500\n",
      "2306/2306 [==============================] - 56s 24ms/step - loss: 5.7287\n",
      "Epoch 6/500\n",
      "2306/2306 [==============================] - 56s 24ms/step - loss: 5.7258\n",
      "Epoch 7/500\n",
      "2306/2306 [==============================] - 52s 23ms/step - loss: 5.7222\n",
      "Epoch 8/500\n",
      "2306/2306 [==============================] - 52s 23ms/step - loss: 5.6982\n",
      "Epoch 9/500\n",
      "2306/2306 [==============================] - 53s 23ms/step - loss: 5.6962\n",
      "Epoch 10/500\n",
      "2306/2306 [==============================] - 53s 23ms/step - loss: 5.6940\n",
      "Epoch 11/500\n",
      "2306/2306 [==============================] - 53s 23ms/step - loss: 5.6632\n",
      "Epoch 12/500\n",
      "2306/2306 [==============================] - 52s 23ms/step - loss: 5.6747\n",
      "Epoch 13/500\n",
      "2306/2306 [==============================] - 55s 24ms/step - loss: 5.6520\n",
      "Epoch 14/500\n",
      "2306/2306 [==============================] - 56s 24ms/step - loss: 5.6181\n",
      "Epoch 15/500\n",
      "2306/2306 [==============================] - 62s 27ms/step - loss: 5.6160\n",
      "Epoch 16/500\n",
      "2306/2306 [==============================] - 73s 32ms/step - loss: 5.5654\n",
      "Epoch 17/500\n",
      "2306/2306 [==============================] - 73s 32ms/step - loss: 5.5521\n",
      "Epoch 18/500\n",
      "2306/2306 [==============================] - 71s 31ms/step - loss: 5.5783\n",
      "Epoch 19/500\n",
      "2306/2306 [==============================] - 67s 29ms/step - loss: 5.5496\n",
      "Epoch 20/500\n",
      "2306/2306 [==============================] - 56s 24ms/step - loss: 5.5791\n",
      "Epoch 21/500\n",
      "2306/2306 [==============================] - 52s 22ms/step - loss: 5.5695\n",
      "Epoch 22/500\n",
      "2306/2306 [==============================] - 52s 23ms/step - loss: 5.5415\n",
      "Epoch 23/500\n",
      "2306/2306 [==============================] - 53s 23ms/step - loss: 5.5227\n",
      "Epoch 24/500\n",
      "2306/2306 [==============================] - 54s 23ms/step - loss: 5.5116\n",
      "Epoch 25/500\n",
      "2306/2306 [==============================] - 53s 23ms/step - loss: 5.5102\n",
      "Epoch 26/500\n",
      "2306/2306 [==============================] - 55s 24ms/step - loss: 5.5112\n",
      "Epoch 27/500\n",
      "2306/2306 [==============================] - 71s 31ms/step - loss: 5.4835\n",
      "Epoch 28/500\n",
      "2306/2306 [==============================] - 70s 31ms/step - loss: 5.4915\n",
      "Epoch 29/500\n",
      "2306/2306 [==============================] - 71s 31ms/step - loss: 5.5190\n",
      "Epoch 30/500\n",
      "2306/2306 [==============================] - 69s 30ms/step - loss: 5.5060\n",
      "Epoch 31/500\n",
      "2306/2306 [==============================] - 69s 30ms/step - loss: 5.4695\n",
      "Epoch 32/500\n",
      "2306/2306 [==============================] - 54s 23ms/step - loss: 5.4543\n",
      "Epoch 33/500\n",
      "2306/2306 [==============================] - 55s 24ms/step - loss: 5.4532\n",
      "Epoch 34/500\n",
      "2306/2306 [==============================] - 57s 25ms/step - loss: 5.4398\n",
      "Epoch 35/500\n",
      "2306/2306 [==============================] - 58s 25ms/step - loss: 5.4521\n",
      "Epoch 36/500\n",
      "2306/2306 [==============================] - 75s 33ms/step - loss: 5.4576\n",
      "Epoch 37/500\n",
      "2306/2306 [==============================] - 75s 32ms/step - loss: 5.4074\n",
      "Epoch 38/500\n",
      "2306/2306 [==============================] - 269s 117ms/step - loss: 5.3958\n",
      "Epoch 39/500\n",
      "2306/2306 [==============================] - 74s 32ms/step - loss: 5.4328\n",
      "Epoch 40/500\n",
      "2306/2306 [==============================] - 79s 34ms/step - loss: 5.4458\n",
      "Epoch 41/500\n",
      "2306/2306 [==============================] - 77s 33ms/step - loss: 5.4608\n",
      "Epoch 42/500\n",
      "2306/2306 [==============================] - 74s 32ms/step - loss: 5.4334\n",
      "Epoch 43/500\n",
      "2306/2306 [==============================] - 67s 29ms/step - loss: 5.4661\n",
      "Epoch 44/500\n",
      "2306/2306 [==============================] - 66s 29ms/step - loss: 5.4235\n",
      "Epoch 45/500\n",
      "2306/2306 [==============================] - 53s 23ms/step - loss: 5.4088\n",
      "Epoch 46/500\n",
      "2306/2306 [==============================] - 51s 22ms/step - loss: 5.3967\n",
      "Epoch 47/500\n",
      "2306/2306 [==============================] - 52s 22ms/step - loss: 5.3826\n",
      "Epoch 48/500\n",
      "2306/2306 [==============================] - 50s 22ms/step - loss: 5.4172\n",
      "Epoch 49/500\n",
      "2306/2306 [==============================] - 51s 22ms/step - loss: 5.4210\n",
      "Epoch 50/500\n",
      "2306/2306 [==============================] - 52s 22ms/step - loss: 5.4141\n",
      "Epoch 51/500\n",
      "2306/2306 [==============================] - 51s 22ms/step - loss: 5.4013\n",
      "Epoch 52/500\n",
      "2306/2306 [==============================] - 51s 22ms/step - loss: 5.4160\n",
      "Epoch 53/500\n",
      "2306/2306 [==============================] - 50s 22ms/step - loss: 5.4031\n",
      "Epoch 54/500\n",
      "2306/2306 [==============================] - 52s 23ms/step - loss: 5.4265\n",
      "Epoch 55/500\n",
      "2306/2306 [==============================] - 53s 23ms/step - loss: 5.4474\n",
      "Epoch 56/500\n",
      "2306/2306 [==============================] - 51s 22ms/step - loss: 5.3995\n",
      "Epoch 57/500\n",
      "2306/2306 [==============================] - 52s 22ms/step - loss: 5.3882\n",
      "Epoch 58/500\n",
      "2306/2306 [==============================] - 74s 32ms/step - loss: 5.4082\n",
      "Epoch 59/500\n",
      "2306/2306 [==============================] - 57s 25ms/step - loss: 5.4277\n",
      "Epoch 60/500\n",
      "2306/2306 [==============================] - 54s 23ms/step - loss: 5.4727\n",
      "Epoch 61/500\n",
      "2306/2306 [==============================] - 58s 25ms/step - loss: 5.4171\n",
      "Epoch 62/500\n",
      "2306/2306 [==============================] - 61s 27ms/step - loss: 5.3906\n",
      "Epoch 63/500\n",
      "2306/2306 [==============================] - 57s 25ms/step - loss: 5.4031\n",
      "Epoch 64/500\n",
      "2306/2306 [==============================] - 61s 26ms/step - loss: 5.3931\n",
      "Epoch 65/500\n",
      "2306/2306 [==============================] - 75s 33ms/step - loss: 5.3620\n",
      "Epoch 66/500\n",
      "2306/2306 [==============================] - 71s 31ms/step - loss: 5.4000\n",
      "Epoch 67/500\n",
      "2306/2306 [==============================] - 71s 31ms/step - loss: 5.3857\n",
      "Epoch 68/500\n",
      "2306/2306 [==============================] - 74s 32ms/step - loss: 5.3546\n",
      "Epoch 69/500\n",
      "2306/2306 [==============================] - 71s 31ms/step - loss: 5.3431\n",
      "Epoch 70/500\n",
      "2306/2306 [==============================] - 70s 30ms/step - loss: 5.3481\n",
      "Epoch 71/500\n",
      "2306/2306 [==============================] - 75s 33ms/step - loss: 5.3435\n",
      "Epoch 72/500\n",
      "2306/2306 [==============================] - 71s 31ms/step - loss: 5.3395\n",
      "Epoch 73/500\n",
      " 384/2306 [===>..........................] - ETA: 1:06 - loss: 5.2904"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " epches代表训练轮次，训练次数越多，模型拟合的越好，尝试训练500次，最终的结果将非常好，\n",
    " 自己电脑估计得跑个一百天，还是得用GPU来跑，否则存在非常大的瓶颈\n",
    " 这里我就进行训练两次看下效果\n",
    "\"\"\"\n",
    "# Train the neural network\n",
    "model.fit([image_data, X], y, batch_size=64, shuffle=False, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-26T04:41:21.778Z"
    }
   },
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-26T04:41:21.779Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'START'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(900):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0][-100:]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # Print the prediction\n",
    "        print(' ' + word, end='')\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'END':\n",
    "            break\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-26T04:41:21.781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and image, preprocess it for IR2, extract features and generate the HTML\n",
    "test_image = img_to_array(load_img('images/87.jpg', target_size=(299, 299)))\n",
    "test_image = np.array(test_image, dtype=float)\n",
    "test_image = preprocess_input(test_image)\n",
    "test_features = IR2.predict(np.array([test_image]))\n",
    "generate_desc(model, tokenizer, np.array(test_features), 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
